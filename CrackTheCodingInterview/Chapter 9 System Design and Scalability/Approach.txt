1. Scope the problem
You can't design a system if you don't know what you're designing. Scoping the problem is important
because you want to ensure that you're building what the interviewer wants and because this might be
something that interviewer is specifically evaluating.

If you're asked something such as "Design TinyURL'; you'll want to understand what exactly you need to
implement. Will people be able to specify their own short URLs? Or will it all be auto-generated? Will you
need to keep track of any stats on the clicks? Should the URLs stay alive forever, or do they have a timeout?

Make a list here as well of the major features or use cases. For example, forTinyURL, it might be:
• Shortening a URL to a TinyURL.
• Analytics for a URL.
• Retrieving the URL associated with a TinyURL.
• User accounts and link management.

2. Make Reasonable Assumptions
It's okay to make some assumptions (when necessary), but they should be reasonable. For example, it
would not be reasonable to assume that your system only needs to process 100 users per day, or to assume
that you have infinite memory available.

However, it might be reasonable to design for a max of one million new URLs per day. Making this assumption
can help you calculate how much data your system might need to store.

3. Draw the Major Components
Get up out of that chair and go to the whiteboard. Draw a diagram of the major components. You might
have something like a frontend server (or set of servers) that pull data from the backend's data store. You
might have another set of servers that crawl the internet for some data, and another set that process
analytics. Draw a picture of what this system might look like.

4. Identify the Key Issues
Once you have a basic design in mind, focus on the key issues. What will be the bottlenecks or major challenges
in the system?
For example, if you were designing TinyURL, one situation you might consider is that while some URLs will
be infrequently accessed, others can suddenly peak. This might happen if a URL is posted on Reddit or
another popular forum. You don't necessarily want to constantly hit the database.
Your interviewer might provide some guidance here. If so, take this guidance and use it.

5. Redesign for the Key Issues
Once you have identified the key issues, it's time to adjust your design for it. You might find that it involves
a major redesign or just some minor tweaking (like using a cache).

Stay up at the whiteboard here and update your diagram as your design changes.
Be open about any limitations in your design. Your interviewer will likely be aware of them, so it's important
to communicate that you're aware of them, too.


-- Algorithms that Scale: Step-By-Step
In some cases, you're not being asked to design an entire system. You're just being asked to design a single
feature or algorithm, but you have to do it in a scalable way. Or, there might be one algorithm part that is
the "real"focus of a broader design question.
In these cases, try the following approach.

1. Ask Questions
As in the earlier approach, ask questions to make sure you really understand the question. There might
be details the interviewer left out (intentionally or unintentionally). You can't solve a problem if you don't
understand exactly what the problem is.

2. Make Believe
Pretend that the data can all fit on one machine and there are no memory limitations. How would you solve
the problem? The answer to this question will provide the general outline for your solution.

3. Get Real
Now go back to the original problem. How much data can you fit on one machine, and what problems will
occur when you split up the data? Common problems include figuring out how to logically divide the data
up, and how one machine would identify where to look up a different piece of data.

4. Solve Problems
Finally, think about how to solve the issues you identified in Step 2. Remember that the solution for each
issue might be to actually remove the issue entirely, or it might be to simply mitigate the issue. Usually, you
can continue using (with modifications) the approach you outlined in Step 1, but occasionally you will need
to fundamentally alter the approach.
Note that an iterative approach is typically useful. That is, once you have solved the problems from Step 3,
new problems may have emerged, and you must tackle those as well.
Your goal is not to re-architect a complex system that companies have spent millions of dollars building,
but rather to demonstrate that you can analyze and solve problems. Poking holes in your own solution is a
fantastic way to demonstrate this.


-- Key Concepts

	-- Horizontal vs. Vertical Scaling
	A system can be scaled one of two ways.
	Vertical scaling means increasing the resources of a specific node. For example, you might add additional
	memory to a server to improve its ability to handle load changes.
	Horizontal scaling means increasing the number of nodes. For example, you might add additional
	servers, thus decreasing the load on any one server.
	Vertical scaling is generally easier than horizontal scaling, but it's limited. You can only add so much memory
	or disk space.

	-- Load Balancer
	Typically, some frontend parts of a scalable website will be thrown behind a load balancer. This allows a
	system to distribute the load evenly so that one server doesn't crash and take down the whole system. To
	do so, of course, you have to build out a network of cloned servers that all have essentially the same code
	and access to the same data.

	-- Database Denormalization and NoSQL
	Joins in a relational database such as SQL can get very slow as the system grows bigger. For this reason, you
	would generally avoid them.

	Denormalization is one part of this. Denormalization means adding redundant information into a database
	to speed up reads. For example, imagine a database describing projects and tasks (where a project can have
	multiple tasks). You might need to get the project name and the task information. Rather than doing a join
	across these tables, you can store the project name within the task table (in addition to the project table).
	Or, you can go with a NoSQL database. A NoSQL database does not support joins and might structure data
	in a different way. It is designed to scale better.

	Or, you can go with a NoSQL database. A NoSQL database does not support joins and might structure data
	in a different way. It is designed to scale better.

	-- Database Partitioning (Sharding)
	Sharding means splitting the data across multiple machines while ensuring you have a way of figuring out
	which data is on which machine.
	
	A few common ways of partitioning include:

	Vertical Partitioning: This is basically partitioning by feature. For example, if you were building a social
	network, you might have one partition for tables relating to profiles, another one for messages, and so
	on. One drawback of this is that if one of these tables gets very large, you might need to repartition that
	database (possibly using a different partitioning scheme).

	Key-Based (or Hash-Based) Partitioning: This uses some part of the data (for example an ID) to partition
	it. A very simple way to do this is to allocate N servers and put the data on mod (key, n). One issue
	with this is that the number of servers you have is effectively fixed. Adding additional servers means
	reallocating all the data-a very expensive task.

	Directory-Based Partitioning: In this scheme, you maintain a lookup table for where the data can be
	found. This makes it relatively easy to add additional servers, but it comes with two major drawbacks.
	First, the lookup table can be a single point of failure. Second, constantly accessing this table impacts
	performance.

	-- Caching
	An in-memory cache can deliver very rapid results. It is a simple key-value pairing and typically sits between
	your application layer and your data store.
	When an application requests a piece of information, it first tries the cache. If the cache does not contain the
	key, it will then look up the data in the data store. (At this point, the data might-or might not-be stored
	in the data store.)
	When you cache, you might cache a query and its results directly. Or, alternatively, you can cache the specific
	object (for example, a rendered version of a part of the website, or a list of the most recent blog posts).

	-- Asynchronous Processing & Queues
	Slow operations should ideally be done asynchronously. Otherwise, a user might get stuck waiting and
	waiting for a process to complete.

	In some cases, we can do this in advance (i.e., we can pre-process). For example, we might have a queue of
	jobs to be done that update some part of the website. If we were running a forum, one of these jobs might
	be to re-render a page that lists the most popular posts and the number of comments. That list might end
	up being slightly out of date, but that's perhaps okay. It's better than a user stuck waiting on the website
	to load simply because someone added a new comment and invalidated the cached version of this page.
	In other cases, we might tell the user to wait and notify them when the process is done. You've probably
	seen this on websites before. Perhaps you enabled some new part of a website and it says it needs a few
	minutes to import your data, but you'll get a notification when it's done.

	-- Networking Metrics
	Bandwidth: This is the maximum amount of data that can be transferred in a unit of time. It is typically
	expressed in bits per second (or some similar ways, such as gigabytes per second).

	Throughput: Whereas bandwidth is the maximum data that can be transferred in a unit of time,
	throughput is the actual amount of data that is transferred.

	Latency: This is how long it takes data to go from one end to the other. That is, it is the delay between the
	sender sending information (even a very small chunk of data) and the receiver receiving it.

	-- MapReduce
	MapReduce is often associated with Google, but it's used much more broadly than that. A MapReduce
	program is typically used to process large amounts of data.
	As its name suggests, a MapReduce program requires you to write a Map step and a Reduce step. The rest
	is handled by the system.

	-- Map takes in some data and emits a <key, value> pair.
	-- Reduce takes a key and a set of associated values and "reduces"them in some way, emitting a new key
	and value. The results of this might be fed back into the Reduce program for more reducing.
	MapReduce allows us to do a lot of processing in parallel, which makes processing huge amounts of data
	more scalable.


-- Considerations
	-- Failures: Essentially any part of a system can fail. You'll need to plan for many or all of these failures.
	-- Availability and Reliability: Availability is a function of the percentage of time the system is operational.
	Reliability is a function of the probability that the system is operational for a certain unit of time.
	-- Read-heavy vs. Write-heavy: Whether an application will do a lot of reads or a lot of writes impacts the
	design. If it's write-heavy, you could consider queuing up the writes (but think about potential failure
	here!). If it's read-heavy, you might want to cache. Other design decisions could change as well.
	-- Security: Security threats can, of course, be devastating for a system. Think about the types of issues a
	system might face and design around those.


-- Example Problem
Given a list of millions of documents, how would you find all documents that contain a list of words? The words
can appear in any order, but they must be complete words. That is, "book" does not match "bookkeeper."

Step 1:
The first step is to pretend we just have a few dozen documents. How would we implement findWords in
this case? (Tip: stop here and try to solve this yourself before reading on.)
One way to do this is to pre-process each document and create a hash table index. This hash table would
map from a word to a list of the documents that contain that word.
"books" -> {doc2, doc3, doc6, doc8}
"many" -> {docl, doc3, doc7, doc8, doc9}
To search for "many books;' we would simply do an intersection on the values for"books" and "many'; and
return {doc 3, doc8} as the result.

Step 2:
Now go back to the original problem. What problems are introduced with millions of documents? For
starters, we probably need to divide up the documents across many machines. Also, depending on a variety
of factors, such as the number of possible words and the repetition of words in a document, we may not be
able to fit the full hash table on one machine. Let's assume that this is the case.

This division introduces the following key concerns:
1. How will we divide up our hash table? We could divide it up by keyword, such that a given machine
contains the full document list for a given word. Or, we could divide by document, such that a machine
contains the keyword mapping for only a subset of the documents.
2. Once we decide how to divide up the data, we may need to process a document on one machine and
push the results off to other machines. What does this process look like? (Note: if we divide the hash
table by document this step may not be necessary.)
3. We will need a way of knowing which machine holds a piece of data. What does this lookup table look
like, and where is it stored?

Step 3:
In Step 3, we find solutions to each of these issues. One solution is to divide up the words alphabetically by
keyword, such that each machine controls a range of words (e.g., "after"through "apple").

We can implement a simple algorithm in which we iterate through the keywords alphabetically, storing as
much data as possible on one machine. When that machine is full, we can move to the next machine.
The advantage of this approach is that the lookup table is small and simple (since it must only specify a
range of values), and each machine can store a copy of the lookup table. However, the disadvantage is that
if new documents or words are added, we may need to perform an expensive shift of keywords.
To find all the documents that match a list of strings, we would first sort the list and then send each machine
a lookup request for the strings that the machine owns. For example, if our string is "after builds
boat amaze banana", machine 1 would get a lookup request for{" after", "amaze"}.
Machine 1 looks up the documents containing "after" and "amaze;' and performs an intersection on these
document lists. Machine 3 does the same for {"banana", "boat", "builds"}, and intersects their
lists.
In the final step, the initial machine would do an intersection on the results from Machine 1 and Machine 3.
The following diagram explains this process.